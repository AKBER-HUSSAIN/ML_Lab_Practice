{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNR2Dp6LcaOCP4dSe+DoZ2N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AKBER-HUSSAIN/ML_Lab_Practice/blob/main/week8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtdtgBV5ZBlt",
        "outputId": "a4d8c465-6388-448d-e9bf-bd8e85fa9c80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Performance:\n",
            "Accuracy: 94.1520\n",
            "Precision: 97.1154\n",
            "Recall: 93.5185\n",
            "F1-Score: 95.2830\n",
            "Decision Tree Accuracy: 94.1520\n",
            "\n",
            "Random Forest Accuracy for different number of estimators:\n",
            "n_estimators = 1: Accuracy = 94.7368\n",
            "n_estimators = 8: Accuracy = 97.0760\n",
            "n_estimators = 10: Accuracy = 96.4912\n",
            "n_estimators = 20: Accuracy = 97.0760\n",
            "n_estimators = 50: Accuracy = 97.0760\n",
            "n_estimators = 100: Accuracy = 97.0760\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train/Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 1. Single Classifier: Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "\n",
        "# Metrics for Decision Tree\n",
        "dt_metrics = {\n",
        "    'Accuracy': accuracy_score(y_test, y_pred_dt)*100,\n",
        "    'Precision': precision_score(y_test, y_pred_dt)*100,\n",
        "    'Recall': recall_score(y_test, y_pred_dt)*100,\n",
        "    'F1-Score': f1_score(y_test, y_pred_dt)*100\n",
        "}\n",
        "\n",
        "print(\"Decision Tree Performance:\")\n",
        "for metric, value in dt_metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "# 2. Ensemble Classifier: Random Forest with varying n_estimators\n",
        "estimators_range = [1, 8, 10, 20, 50, 100]\n",
        "rf_metrics = {'n_estimators': [], 'Accuracy': [], 'Precision': [], 'Recall': [], 'F1-Score': []}\n",
        "\n",
        "for n in estimators_range:\n",
        "    rf = RandomForestClassifier(n_estimators=n, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "    rf_metrics['n_estimators'].append(n)\n",
        "    rf_metrics['Accuracy'].append(accuracy_score(y_test, y_pred_rf))\n",
        "    rf_metrics['Precision'].append(precision_score(y_test, y_pred_rf))\n",
        "    rf_metrics['Recall'].append(recall_score(y_test, y_pred_rf))\n",
        "    rf_metrics['F1-Score'].append(f1_score(y_test, y_pred_rf))\n",
        "\n",
        "\n",
        "# Print accuracy for Decision Tree\n",
        "print(f\"Decision Tree Accuracy: {dt_metrics['Accuracy']:.4f}\")\n",
        "\n",
        "# Print accuracy for Random Forest with different n_estimators\n",
        "print(\"\\nRandom Forest Accuracy for different number of estimators:\")\n",
        "for n, acc in zip(rf_metrics['n_estimators'], rf_metrics['Accuracy']):\n",
        "    print(f\"n_estimators = {n}: Accuracy = {acc*100:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What differences do you observe between the Decision Tree and Random Forest results?\n",
        "\n",
        "Decision tree :\n",
        "-> It uses only single tree that splits data on features\n",
        "-> using decison tree model can be overfitted\n",
        "-> small change in random_state leads to diffrence in result of accuracy\n",
        "\n",
        "\n",
        "Random Forest Classifier :\n",
        "-> uses multiple decison trees\n",
        "-> combines prediction of multiple decision trees\n",
        "-> it does nnot overfir or underfit the model because of usng multiple decion trees\n",
        "-> It takes average of all the deciosn tree so it ancel outs and inaccurate prediction"
      ],
      "metadata": {
        "id": "aACAMj5_hCYJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. How does increasing the number of estimators affect performance and stability?\n",
        "\n",
        "->The performance (accuracy, precision, recall, F1) usually improves up to a point.\n",
        "-> Variance and instability in predictions decrease because averaging many trees smooths out individual errors.\n",
        "-> after some certain no of trees the accuracy does not changes\n"
      ],
      "metadata": {
        "id": "vJon2351icN5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Why does Random Forest generally perform better than a single Decision Tree?\n",
        "\n",
        "-> Single decision trees can be very sensitive to the data they are trained on (high variance). Random Forest reduces this by averaging many trees trained on different data subsets.\n",
        "\n",
        "-> Each tree in Random Forest considers a random subset of features, ensuring trees are diverse and not all making the same mistakes.\n",
        "\n",
        "-> Averaging many weak learners (trees) results in a strong learner that generalizes better to unseen data."
      ],
      "metadata": {
        "id": "gHl9V6sLi0h_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from scipy.stats import mode\n",
        "\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "clf1 = LogisticRegression(max_iter=10000, random_state=42)\n",
        "clf2 = DecisionTreeClassifier(random_state=42)\n",
        "clf3 = KNeighborsClassifier()\n",
        "\n",
        "\n",
        "clf1.fit(X_train, y_train)\n",
        "clf2.fit(X_train, y_train)\n",
        "clf3.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "pred1 = clf1.predict(X_test)\n",
        "pred2 = clf2.predict(X_test)\n",
        "pred3 = clf3.predict(X_test)\n",
        "\n",
        "\n",
        "prob1 = clf1.predict_proba(X_test)\n",
        "prob2 = clf2.predict_proba(X_test)\n",
        "prob3 = clf3.predict_proba(X_test)\n",
        "\n",
        "\n",
        "preds = np.array([pred1, pred2, pred3])\n",
        "max_voting = mode(preds, axis=0, keepdims=False).mode\n",
        "\n",
        "\n",
        "avg_prob = (prob1 + prob2 + prob3) / 3\n",
        "avg_voting = np.argmax(avg_prob, axis=1)\n",
        "\n",
        "\n",
        "acc1 = accuracy_score(y_test, pred1)\n",
        "acc2 = accuracy_score(y_test, pred2)\n",
        "acc3 = accuracy_score(y_test, pred3)\n",
        "\n",
        "weights = np.array([acc1, acc2, acc3])\n",
        "total_weight = np.sum(weights)\n",
        "weighted_probs = (prob1 * acc1 + prob2 * acc2 + prob3 * acc3) / total_weight\n",
        "weighted_avg_voting = np.argmax(weighted_probs, axis=1)\n",
        "\n",
        "\n",
        "def evaluate(name, y_true, y_pred):\n",
        "    print(f\"\\n{name} Results:\")\n",
        "    print(f\"Accuracy:  {accuracy_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"Precision: {precision_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"Recall:    {recall_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"F1-Score:  {f1_score(y_true, y_pred):.4f}\")\n",
        "\n",
        "evaluate(\"Max Voting\", y_test, max_voting)\n",
        "evaluate(\"Average Voting\", y_test, avg_voting)\n",
        "evaluate(\"Weighted Average Voting\", y_test, weighted_avg_voting)\n"
      ],
      "metadata": {
        "id": "9rxlh6MTcbwR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from scipy.stats import mode\n",
        "\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "\n",
        "model1 = DecisionTreeClassifier(random_state=1)\n",
        "model2 = LogisticRegression(max_iter=1000, random_state=1)\n",
        "model3 = KNeighborsClassifier()\n",
        "\n",
        "model1.fit(X_train, y_train)\n",
        "model2.fit(X_train, y_train)\n",
        "model3.fit(X_train, y_train)\n",
        "\n",
        "# 1. Hard Voting (majority vote)\n",
        "\n",
        "pred1 = model1.predict(X_test)\n",
        "pred2 = model2.predict(X_test)\n",
        "pred3 = model3.predict(X_test)\n",
        "\n",
        "\n",
        "predictions = np.vstack((pred1, pred2, pred3)).T\n",
        "\n",
        "\n",
        "hard_vote_preds, _ = mode(predictions, axis=1)\n",
        "\n",
        "hard_vote_preds = hard_vote_preds.ravel()\n",
        "\n",
        "\n",
        "hard_vote_accuracy = accuracy_score(y_test, hard_vote_preds)\n",
        "\n",
        "\n",
        "prob1 = model1.predict_proba(X_test)\n",
        "prob2 = model2.predict_proba(X_test)\n",
        "prob3 = model3.predict_proba(X_test)\n",
        "\n",
        "\n",
        "avg_prob = (prob1 + prob2 + prob3) / 3\n",
        "\n",
        "\n",
        "soft_vote_preds = np.argmax(avg_prob, axis=1)\n",
        "\n",
        "\n",
        "soft_vote_accuracy = accuracy_score(y_test, soft_vote_preds)\n",
        "\n",
        "print(\"Accuracy of Decision Tree:\", accuracy_score(y_test, pred1))\n",
        "print(\"Accuracy of Logistic Regression:\", accuracy_score(y_test, pred2))\n",
        "print(\"Accuracy of KNN:\", accuracy_score(y_test, pred3))\n",
        "print(\"\\nAccuracy of Manual Hard Voting:\", hard_vote_accuracy)\n",
        "print(\"Accuracy of Manual Soft Voting:\", soft_vote_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SP0IlDcolSJ2",
        "outputId": "bdd71373-844b-4349-8d12-62c3de51ebe1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Decision Tree: 0.9555555555555556\n",
            "Accuracy of Logistic Regression: 0.9777777777777777\n",
            "Accuracy of KNN: 0.9777777777777777\n",
            "\n",
            "Accuracy of Manual Hard Voting: 0.9555555555555556\n",
            "Accuracy of Manual Soft Voting: 0.9555555555555556\n"
          ]
        }
      ]
    }
  ]
}